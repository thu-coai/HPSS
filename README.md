# HPSS: Heuristic Prompting Strategy Search for LLM-as-a-Judge

This is the official implementation of HPSS: Heuristic Prompting Strategy Search for LLM-as-a-Judge. 

## âš™ï¸ Requirements

To install requirements:

```setup
pip install -r requirements.txt
```

## ğŸŒ³ File Structure

```plaintext
HPSS
â”œâ”€â”€ auxiliary
â”‚Â Â  â”œâ”€â”€ hanna
â”‚Â Â  â”œâ”€â”€ sfhot
â”‚Â Â  â”œâ”€â”€ sfres
â”‚Â Â  â”œâ”€â”€ summeval
â”‚Â Â  â””â”€â”€ topical_chat
â”œâ”€â”€ baseline
â”‚Â Â  â”œâ”€â”€ ape_prompts
â”‚Â Â  â”œâ”€â”€ ape.py
â”‚Â Â  â”œâ”€â”€ greedy.py
â”‚Â Â  â”œâ”€â”€ opro_prompt
â”‚Â Â  â””â”€â”€ opro.py
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ hanna
â”‚Â Â  â”œâ”€â”€ sfhot
â”‚Â Â  â”œâ”€â”€ sfres
â”‚Â Â  â”œâ”€â”€ summeval
â”‚Â Â  â””â”€â”€ topical_chat
â”œâ”€â”€ evaluation
â”‚Â Â  â”œâ”€â”€ scripts
â”‚Â Â  â”œâ”€â”€ dataset2aspects.py
â”‚Â Â  â””â”€â”€ evaluation_aspects.py
â”œâ”€â”€ initiation
â”‚Â Â  â”œâ”€â”€ metrics
â”‚Â Â  â”œâ”€â”€ scripts
â”‚Â Â  â”œâ”€â”€ dataset2aspects.py
â”‚Â Â  â””â”€â”€ judge_vllm_aspects.py
â”œâ”€â”€ prompts
â”‚Â Â  â”œâ”€â”€ initiation
â”‚Â Â  â””â”€â”€ search
â”œâ”€â”€ README.md
â”œâ”€â”€ args.py
â”œâ”€â”€ dataset2aspects.py
â”œâ”€â”€ factors.py
â”œâ”€â”€ hpss.py
â”œâ”€â”€ item.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ utils.py
â”œâ”€â”€ validation.py
â””â”€â”€ vllm_inference.py
```

`auxiliary/`: Auxiliary information for factors `Autocot`, `Evaluation Criteria`, and `In-Context Example`.

`baseline/`: Existing automatic prompt optimization methods we implement.

`data/`: The validation and test dataset. Auxiliary information for factors `Reference` and `Metrics` are placed in each data sample.

`evaluation/`: Scripts for calculating the correlation coefficient.

`inference/`: Scripts for utilizing LLM-as-a-Judge to text evaluation.

`initiation/`: The implementation of the **Initialization** stage in our algorithm.

`prompts/`: Prompt templates used in HPSS.

`hpss.py`: The implementation of the **Iterative Search** stage in our algorithm.

## ğŸš€ Running HPSS

We have placed the results of the **Initialization** stage in `initiation/metrics/` for each dataset. You can run the following script to run HPSS to search for well-behaved prompting strategies.

```bash
python hpss.py \
    --model <model> \
    --dataset <dataset> \
    --budget <budget> \
    --beam_size <beam_size> \
    --seed <seed> \
    --temperature <temperature>
    --lambda_ <lambda_> \
    --rho <rho> \
    --g <g> \
    --initiation_path <initiation_path> \
    --output_path <output_path> \
    --aspect <aspect>
```

Here is an example:

```bash
python hpss.py
    --model qwen_2_5_14b \
    --dataset topical_chat \
    --budget 50 \
    --beam_size 5 \
    --seed 42 \
    --temperature 5.0 \
    --lambda_ 4.0 \
    --rho 0.2 \
    --g 2 \
    --initiation_path initiation/metrics \
    --output_path hpss_results \
    --aspect coherence \
```

After running the scripts, you can find the results saved in `hpss_results/`.

